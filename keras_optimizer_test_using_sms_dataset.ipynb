{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 4248 2021-2022 \n",
    "## les membres du groupe\n",
    "- fezeu Ghomsi Eugene Clotaire 17Q2864 \n",
    "- Nono Mabou Wilfried Brondon 21S2817 brondonnono3@gmail.com\n",
    "- NGBAYAFOU NGOUH Chéripha Cheriphalynette@gmail.com\n",
    "- Junior Libii junior.libii@facsciences-uy1.cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  classification de spam avec un dataset sms \n",
    "on utilise ici l'algorithme doctovec pour transformer les données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test d'existe et ou récupération de la dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recupération de la dataset\n",
    "import os.path\n",
    "from os import path\n",
    "import getpass\n",
    "filepath = ''\n",
    "REPOPATH = 'fez2000/TP_4248.git'\n",
    "FILENAME= 'spam.csv'\n",
    "HOST='github.com'\n",
    "if path.exists(f'./{FILENAME}'):\n",
    "    filepath = f'./{FILENAME}'\n",
    "elif path.exists(f'./datas/{FILENAME}'):\n",
    "    filepath = f'./datas/{FILENAME}'\n",
    "else:\n",
    "    filepath = f'./datas/{FILENAME}'\n",
    "    def load_from_git():\n",
    "        def clone(): \n",
    "            GIT_PATH = f'https://{HOST}/{REPOPATH}' \n",
    "            !git clone {GIT_PATH} 'temp'\n",
    "        def clone_by_token():\n",
    "            GIT_TOKEN= getpass.getpass(\"votre token git\") \n",
    "            GIT_PATH = f'https://{GIT_TOKEN}@{HOST}/{REPOPATH}' \n",
    "            !git clone {GIT_PATH} 'temp'\n",
    "        def clone_by_username_and_password():\n",
    "            GIT_USER_NAME= getpass.getpass(\"votre nom d'utilisateur git\")\n",
    "            GIT_PASSWORD= getpass.getpass(\"votre mot de pass git\")\n",
    "            GIT_PATH = f'https://{GIT_USER_NAME}:{GIT_PASSWORD}@{HOST}/{REPOPATH}'\n",
    "            !git  clone  {GIT_PATH} 'temp' \n",
    "        if not path.exists(f'./temp/datas/{FILENAME}'):\n",
    "            try:\n",
    "                clone()\n",
    "            except e:\n",
    "                pass\n",
    "        if not path.exists(f'./temp/datas/{FILENAME}'):\n",
    "            try:\n",
    "                clone_by_username_and_password()\n",
    "            except e:\n",
    "                pass\n",
    "        if not path.exists(f'./temp/datas/{FILENAME}'):\n",
    "            try:\n",
    "                clone_by_token()\n",
    "            except e:\n",
    "                pass\n",
    "        if not path.exists(f'./temp/datas/{FILENAME}'):     \n",
    "            load_from_git()\n",
    "        else:\n",
    "            %cd './temp'\n",
    "\n",
    "    load_from_git()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /home/eugene/anaconda3/lib/python3.9/site-packages (6.4.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipykernel) (1.4.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipykernel) (6.1.12)\n",
      "Requirement already satisfied: ipython-genutils in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipykernel) (0.2.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipykernel) (6.1)\n",
      "Requirement already satisfied: ipython<8.0,>=7.23.1 in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipykernel) (7.29.0)\n",
      "Requirement already satisfied: traitlets<6.0,>=4.1.0 in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipykernel) (5.1.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipykernel) (0.1.2)\n",
      "Requirement already satisfied: backcall in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: pygments in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel) (58.0.4)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel) (0.18.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel) (3.0.20)\n",
      "Requirement already satisfied: pickleshare in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel) (0.7.5)\n",
      "Requirement already satisfied: decorator in /home/eugene/anaconda3/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel) (5.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/eugene/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel) (0.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /home/eugene/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel) (4.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/eugene/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/eugene/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel) (22.2.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/eugene/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython<8.0,>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/eugene/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/eugene/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel) (1.16.0)\n",
      "Installed kernelspec venv in /home/eugene/.local/share/jupyter/kernels/venv\n",
      "Requirement already satisfied: gensim in /home/eugene/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/eugene/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/eugene/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/eugene/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: seaborn in /home/eugene/anaconda3/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/eugene/anaconda3/lib/python3.9/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/eugene/anaconda3/lib/python3.9/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/eugene/anaconda3/lib/python3.9/site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/eugene/anaconda3/lib/python3.9/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/eugene/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/eugene/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/eugene/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/eugene/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/eugene/anaconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (8.4.0)\n",
      "Requirement already satisfied: six in /home/eugene/anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/eugene/anaconda3/lib/python3.9/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: pandas in /home/eugene/anaconda3/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/eugene/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/eugene/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/eugene/anaconda3/lib/python3.9/site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/eugene/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /home/eugene/anaconda3/lib/python3.9/site-packages (1.20.3)\n",
      "Requirement already satisfied: bs4 in /home/eugene/anaconda3/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/eugene/anaconda3/lib/python3.9/site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/eugene/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: BeautifulSoup4 in /home/eugene/anaconda3/lib/python3.9/site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/eugene/anaconda3/lib/python3.9/site-packages (from BeautifulSoup4) (2.2.1)\n",
      "Requirement already satisfied: lxml in /home/eugene/anaconda3/lib/python3.9/site-packages (4.6.3)\n",
      "Requirement already satisfied: nltk in /home/eugene/anaconda3/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: click in /home/eugene/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/eugene/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/eugene/anaconda3/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /home/eugene/anaconda3/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "/home/eugene/anaconda3/lib/python3.9/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "!pip install --user ipykernel\n",
    "!python -m ipykernel install --user --name=venv\n",
    "!pip install gensim\n",
    "!pip install seaborn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install bs4\n",
    "!pip install BeautifulSoup4\n",
    "!pip install lxml\n",
    "!pip install nltk\n",
    "!python -m nltk.downloader all-nltk\n",
    "!pip install tqdm\n",
    "!pip install tensorflow \n",
    "!pip install textfixtures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "dataset = pd.read_csv(filepath, encoding=\"ISO-8859-1\", usecols=['v1', 'v2'], dtype={'v1':str,'v2':str})\n",
    "dataset = dataset.dropna(how=\"any\")\n",
    "dataset.rename(columns={'v2':'corpus', \"v1\": \"target\"}, inplace=True)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['corpus'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on a 86961  words, c'est un petit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_pro = dataset['target'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.9)\n",
    "plt.ylabel('Nombre d\\'occurence', fontsize=12)\n",
    "plt.xlabel('Messages', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "Below we define a function to convert text to lower-case and strip punctuation/symbols from words and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "dataset['corpus'] = dataset['corpus'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['corpus'][20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test split of 70/30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "train = pd.DataFrame(train)\n",
    "test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['corpus']), tags=[r.target]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['corpus']), tags=[r.target]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what a training entry looks like - an example complaint narrative tagged by 'Credit reporting'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged.values[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We'll instantiate a Doc2Vec model-Distributed Bag of Words (DBOW). In the Word2Vec architecture, the two algorithm names are “continuous bag of words” (cbow) and “skip-gram” (sg); in the Doc2Vec architecture, the corresponding algorithms are “distributed bag of words” (dbow) and “distributed memory” (dm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBOW\n",
    "\n",
    "DBOW is the Doc2Vec model analogous to Skip-gram model in Word2Vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "Training a Doc2Vec model is rather straight forward in Gensim, we initialize the model and train for 30 epochs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the minimum word count to 2 in order to discard words with very few occurrences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=1, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buliding the final vector feature for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, epochs=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "input_dim = pd.DataFrame(X_train).shape  # Number of features\n",
    "adam = { 'optimizer': tf.keras.optimizers.Adam(learning_rate=0.1), 'name': 'Adam' }\n",
    "sgd = { 'optimizer': tf.keras.optimizers.SGD(learning_rate=0.1), 'name': 'SGD' }\n",
    "sgdnesterov = { 'optimizer': tf.keras.optimizers.SGD(learning_rate=0.1, nesterov=True), 'name': 'SGD_nesterov' }\n",
    "optimizers = [adam, sgd, sgdnesterov]\n",
    "inputs = tf.keras.Input(shape=input_dim)\n",
    "dense = layers.Dense(64, kernel_initializer='uniform', activation=\"relu\")\n",
    "x = dense(inputs)\n",
    "outputs = layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "models = []\n",
    "y_preds = []\n",
    "for optimizer in optimizers:\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"model_{optimizer['name']}\")\n",
    "    model.compile( optimizer=optimizer['optimizer'], metrics=['accuracy'], loss='binary_crossentropy')\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    models.add(model)\n",
    "    y_preds.add(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Memory with Averaging\n",
    "\n",
    "Distributed Memory (DM) acts as a memory that remembers what is missing from the current context — or as the topic of the paragraph. While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document.\n",
    "We again instantiate a Doc2Vec model with a vector size with 300 words and iterating over the training corpus 30 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=5, negative=1, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, epochs=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = get_vectors(new_model, train_tagged)\n",
    "y_test, X_test = get_vectors(new_model, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
